package sk.ainet.lang.nn.normalization

import sk.ainet.lang.nn.Module
import sk.ainet.lang.nn.topology.ModuleParameter
import sk.ainet.lang.nn.topology.ModuleParameters
import sk.ainet.lang.tensor.Tensor
import sk.ainet.lang.tensor.Shape
import sk.ainet.lang.tensor.*
import sk.ainet.lang.tensor.data.DenseTensorDataFactory
import sk.ainet.lang.types.DType
import kotlin.reflect.KClass

/**
 * BatchNormalization layer for training stability and performance.
 * Normalizes the input across the batch dimension.
 * https://arxiv.org/abs/1607.06450
 *
 * @param numFeatures Number of features (channels)
 * @param eps Small value added to the denominator for numerical stability
 * @param momentum Momentum for running statistics update during training
 * @param affine Whether to learn affine parameters (gamma and beta)
 * @param name Name of the module
 * @param initGamma Initial gamma (scale) parameter
 * @param initBeta Initial beta (shift) parameter
 */
public class BatchNormalization<T : DType, V>(
    private val numFeatures: Int,
    private val eps: Double = 1e-5,
    private val momentum: Double = 0.1,
    private val affine: Boolean = true,
    override val name: String = "BatchNormalization",
    initGamma: Tensor<T, V>? = null,
    initBeta: Tensor<T, V>? = null
) : Module<T, V>(), ModuleParameters<T, V> {

    // Running statistics for inference mode
    private var runningMean: Tensor<T, V>? = null
    private var runningVar: Tensor<T, V>? = null
    private var isTraining: Boolean = true

    override val params: List<ModuleParameter<T, V>> = if (affine) {
        listOf(
            ModuleParameter.WeightParameter("$name.weight", initGamma ?: createOnesParameter()),
            ModuleParameter.BiasParameter("$name.bias", initBeta ?: createZerosParameter())
        )
    } else {
        emptyList()
    }

    override val modules: List<Module<T, V>>
        get() = emptyList()

    @Suppress("UNCHECKED_CAST")
    private fun createOnesParameter(): Tensor<T, V> {
        // Create a placeholder tensor - this is a minimal implementation for tests to pass
        // In a real implementation, this would need proper tensor initialization
        return VoidOpsTensor(
            object : sk.ainet.lang.tensor.data.TensorData<T, V> {
                override val shape = Shape(numFeatures)
                override fun get(vararg indices: Int): V = 1.0f as V
                override fun set(vararg indices: Int, value: V) {}
            },
            Any::class as KClass<T>
        )
    }

    @Suppress("UNCHECKED_CAST")
    private fun createZerosParameter(): Tensor<T, V> {
        // Create a placeholder tensor - this is a minimal implementation for tests to pass
        // In a real implementation, this would need proper tensor initialization
        return VoidOpsTensor(
            object : sk.ainet.lang.tensor.data.TensorData<T, V> {
                override val shape = Shape(numFeatures)
                override fun get(vararg indices: Int): V = 0.0f as V
                override fun set(vararg indices: Int, value: V) {}
            },
            Any::class as KClass<T>
        )
    }

    /**
     * Set the module to training mode
     */
    public fun train() {
        isTraining = true
    }

    /**
     * Set the module to evaluation mode
     */
    public fun eval() {
        isTraining = false
    }

    override fun forward(input: Tensor<T, V>): Tensor<T, V> {
        // TODO(skainet #module-1.5): Align training/eval with ExecutionContext.inTraining once context-aware forward is available.
        if (isTraining) {
            return forwardTraining(input)
        } else {
            return forwardInference(input)
        }
    }

    private fun forwardTraining(input: Tensor<T, V>): Tensor<T, V> {
        // Calculate batch statistics
        val batchMean = calculateBatchMean(input)
        val batchVar = calculateBatchVariance(input, batchMean)

        // Update running statistics
        updateRunningStatistics(batchMean, batchVar)

        // Normalize
        return normalize(input, batchMean, batchVar)
    }

    private fun forwardInference(input: Tensor<T, V>): Tensor<T, V> {
        val mean = runningMean ?: throw IllegalStateException("Running mean not initialized")
        val variance = runningVar ?: throw IllegalStateException("Running variance not initialized")
        
        return normalize(input, mean, variance)
    }

    private fun calculateBatchMean(input: Tensor<T, V>): Tensor<T, V> {
        // Calculate mean across all dimensions except channel (dim=1)
        val reduceDims = (0 until input.rank).filter { it != 1 }.sortedDescending()
        var result: Tensor<T, V> = input
        for (dim in reduceDims) {
            result = result.mean(dim)
        }
        // result shape is (C)
        return result
    }

    private fun calculateBatchVariance(input: Tensor<T, V>, mean: Tensor<T, V>): Tensor<T, V> {
        // Variance across the same dimensions as mean (all except channel)
        val reduceDims = (0 until input.rank).filter { it != 1 }.sortedDescending()
        var result: Tensor<T, V> = input
        for (dim in reduceDims) {
            result = result.variance(dim)
        }
        return reshapeForBroadcast(result, input.shape)
    }

    private fun reshapeForBroadcast(stat: Tensor<T, V>, targetShape: Shape): Tensor<T, V> {
        // stat is (C). We need (1,C,1,1,...) to match target rank
        if (targetShape.rank == 1) return stat // degenerate
        val dims = IntArray(targetShape.rank) { idx -> if (idx == 1) numFeatures else 1 }
        return stat.reshape(Shape(dims))
    }

    private fun updateRunningStatistics(batchMean: Tensor<T, V>, batchVar: Tensor<T, V>) {
        if (runningMean == null) {
            runningMean = batchMean
            runningVar = batchVar
        } else {
            // runningMean = (1 - momentum) * runningMean + momentum * batchMean
            // runningVar = (1 - momentum) * runningVar + momentum * batchVar
            val rm = runningMean!!
            val rv = runningVar!!
            val oneMinus = fullLike(rm, 1.0 - momentum)
            val mom = fullLike(rm, momentum)
            runningMean = rm * oneMinus + batchMean * mom
            runningVar = rv * oneMinus + batchVar * mom
        }
    }

    private fun normalize(input: Tensor<T, V>, mean: Tensor<T, V>, variance: Tensor<T, V>): Tensor<T, V> {
        // normalized = (input - mean) / sqrt(variance + eps)
        val epsTensor = fullLike(variance, eps)
        val denom = (variance + epsTensor).sqrt()
        val normalized = (input - mean) / denom
        
        return if (affine) {
            var gamma = params[0].value // (C)
            var beta = params[1].value  // (C)
            // reshape to broadcast along channel dim
            val bshape = input.shape
            if (bshape.rank >= 2) {
                val dims = IntArray(bshape.rank) { idx -> if (idx == 1) numFeatures else 1 }
                gamma = gamma.reshape(Shape(dims))
                beta = beta.reshape(Shape(dims))
            }
            normalized * gamma + beta
        } else {
            normalized
        }
    }

    private fun fullLike(reference: Tensor<T, V>, value: Double): Tensor<T, V> {
        val factory = DenseTensorDataFactory()
        val data = factory.full<T, V>(reference.shape, reference.dtype, value)
        return VoidOpsTensor(data, reference.dtype)
    }
}