package sk.ainet.lang.tensor

import sk.ainet.lang.types.DType

// Tensor extension functions that delegate to the ops component
public fun <T : DType, V> Tensor<T, V>.t(): Tensor<T, V> = ops.transpose(this)
public fun <T : DType, V> Tensor<T, V>.matmul(other: Tensor<T, V>): Tensor<T, V> = ops.matmul(this, other)
public fun <T : DType, V> Tensor<T, V>.flatten(startDim: Int = 0, endDim: Int = -1): Tensor<T, V> = 
    ops.flatten(this, startDim, endDim)

// Operator overloads
public operator fun <T : DType, V> Tensor<T, V>.plus(other: Tensor<T, V>): Tensor<T, V> = ops.add(this, other)
public operator fun <T : DType, V> Tensor<T, V>.minus(other: Tensor<T, V>): Tensor<T, V> = ops.subtract(this, other)
public operator fun <T : DType, V> Tensor<T, V>.times(other: Tensor<T, V>): Tensor<T, V> = ops.multiply(this, other)
public operator fun <T : DType, V> Tensor<T, V>.div(other: Tensor<T, V>): Tensor<T, V> = ops.divide(this, other)

// Tensor op Number (scalar) overloads
public operator fun <T : DType, V> Tensor<T, V>.plus(v: Number): Tensor<T, V> = ops.addScalar(this, v)
public operator fun <T : DType, V> Tensor<T, V>.minus(v: Number): Tensor<T, V> = ops.subScalar(this, v)
public operator fun <T : DType, V> Tensor<T, V>.times(v: Number): Tensor<T, V> = ops.mulScalar(this, v)
public operator fun <T : DType, V> Tensor<T, V>.div(v: Number): Tensor<T, V> = ops.divScalar(this, v)

// Number (scalar) op Tensor overloads
public operator fun <T : DType, V> Number.plus(t: Tensor<T, V>): Tensor<T, V> = t.ops.addScalar(t, this)
public operator fun <T : DType, V> Number.minus(t: Tensor<T, V>): Tensor<T, V> = t.ops.rsubScalar(this, t)
public operator fun <T : DType, V> Number.times(t: Tensor<T, V>): Tensor<T, V> = t.ops.mulScalar(t, this)
public operator fun <T : DType, V> Number.div(t: Tensor<T, V>): Tensor<T, V> = t.ops.rdivScalar(this, t)

// Additional convenience functions
public fun <T : DType, V> Tensor<T, V>.reshape(newShape: Shape): Tensor<T, V> = ops.reshape(this, newShape)
public fun <T : DType, V> Tensor<T, V>.relu(): Tensor<T, V> = ops.relu(this)
public fun <T : DType, V> Tensor<T, V>.sigmoid(): Tensor<T, V> = ops.sigmoid(this)
public fun <T : DType, V> Tensor<T, V>.silu(): Tensor<T, V> = ops.silu(this)
public fun <T : DType, V> Tensor<T, V>.gelu(): Tensor<T, V> = ops.gelu(this)
public fun <T : DType, V> Tensor<T, V>.softmax(dim: Int = -1): Tensor<T, V> = ops.softmax(this, dim)
public fun <T : DType, V> Tensor<T, V>.sum(dim: Int? = null): Tensor<T, V> = ops.sum(this, dim)
public fun <T : DType, V> Tensor<T, V>.mean(dim: Int? = null): Tensor<T, V> = ops.mean(this, dim)
public fun <T : DType, V> Tensor<T, V>.variance(dim: Int? = null): Tensor<T, V> = ops.variance(this, dim)
public fun <T : DType, V> Tensor<T, V>.sqrt(): Tensor<T, V> = ops.sqrt(this)
public fun <T : DType, V> Tensor<T, V>.tril(k: Int = 0): Tensor<T, V> = ops.tril(this, k)

// Global matmul function for the Linear layer usage pattern (removed due to duplicate with extension function)